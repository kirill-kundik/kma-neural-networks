{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset \n",
    "As part of HW2, you will work with the audio dataset. Download the dataset by the following [link](https://urbansounddataset.weebly.com/download-urbansound8k.html) and remember a path to it. You will have to train a sound classification model, explore different audio transformation methods and try different model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanSoundDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 annotations_file,\n",
    "                 audio_dir,\n",
    "                 transformation,\n",
    "                 target_sample_rate,\n",
    "                 num_samples,\n",
    "                 device):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        return signal, label\n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n",
    "        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[\n",
    "            index, 0])\n",
    "        return path\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset \n",
    "ANNOTATIONS_FILE = \"YOUR_PATH/UrbanSound8K/metadata/UrbanSound8K.csv\"\n",
    "AUDIO_DIR = \"YOUR_PATH/UrbanSound8K/audio\"\n",
    "\n",
    "# audio signal sample rate \n",
    "SAMPLE_RATE = 22050\n",
    "# max number of samples in audio\n",
    "NUM_SAMPLES = 22050\n",
    "\n",
    "# optimizer learning rate\n",
    "LEARNING_RATE = 1e-5\n",
    "# number of train epochs\n",
    "EPOCHS = 10\n",
    "# number of samples in each batch size\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining audio preprocessing function. Refer to torchaudio.transforms to \n",
    "# check Spectrogram(), MFCC() and LFCC() transfrormations. Retrain the model\n",
    "# with each transformation and write your metrics resulst and conclusaions.\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_fft=1024,\n",
    "        hop_length=512,\n",
    "        n_mels=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining dataset class with \n",
    "dataset = UrbanSoundDataset(\n",
    "    ANNOTATIONS_FILE,\n",
    "    AUDIO_DIR,\n",
    "    mel_spectrogram,\n",
    "    SAMPLE_RATE,\n",
    "    NUM_SAMPLES,\n",
    "    device\n",
    ")\n",
    "\n",
    "# calculating validation split sizes\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "val_test_size = len(dataset) - train_size\n",
    "\n",
    "val_size = val_test_size // 2\n",
    "test_size = val_test_size - val_size\n",
    "\n",
    "print('Dataset len: ', len(dataset))\n",
    "print(f'Train split: {train_size} | Validation split: {val_size} | Test split: {test_size}')\n",
    "\n",
    "# splitting original dataset into train and val_test sets\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size + test_size]\n",
    ")\n",
    "# splitting val_test set into val dataset and test dataset\n",
    "val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    val_dataset, [val_size, test_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, label = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Define your model architecture here. \n",
    "\n",
    "1) Build a baseline model that consists of consecutive convolution blocks. Flatten the convolution output and pass through the linear layer with softmax activation function, to obtain class distributions\n",
    "2) Add the BatchNorm layer after each convolution block and compare the results with the baseline model\n",
    "3) Add a Dropout layer after each BatchNorm block and compare the results\n",
    "4) Try different parameters for the blocks:\n",
    " - Conv layer: out_channels, kernel_size, stride\n",
    " - Dropout: p\n",
    " and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class CLModel to define your model architecture and computational graph\n",
    "class CLModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ################\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # define you convolution based classification model\n",
    "        # 4 conv blocks / flatten / linear / softmax\n",
    "\n",
    "        ################\n",
    "\n",
    "    def forward(self, input_data: torch.Tensor) -> torch.Tensor:\n",
    "        ################\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # define the logic of your computational graph\n",
    "\n",
    "        ################\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CLModel()\n",
    "from torchsummary import summary\n",
    "summary(cnn, (1, 64, 44))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(data, batch_size:int, shuffle:bool=False) -> torch.utils.data.DataLoader:\n",
    "    ''' creating dataloader  \n",
    "            Arguments:\n",
    "                batch_size: int\n",
    "                    number of samples to process for the model\n",
    "                shuffle: bool\n",
    "                    whether to shuffle dataset. \n",
    "    '''\n",
    "    dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader\n",
    "\n",
    "def train_single_epoch(\n",
    "    model:nn.Module, \n",
    "    data_loader:torch.utils.data.DataLoader, \n",
    "    loss_fn:torch.nn.modules.loss, \n",
    "    optimiser:torch.optim, \n",
    "    device:torch.device):\n",
    "    ''' method to perform single epoch of training '''\n",
    "\n",
    "    rolling_loss = 0. \n",
    "    rolling_metric = 0.\n",
    "    for input, target in data_loader:\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        ################\n",
    "\n",
    "        # YOUR CODE HERE \n",
    "\n",
    "        # step 1: pass input tensor through the model\n",
    "\n",
    "        # step 2: compute loss\n",
    "\n",
    "        # step 3: update optimizer and do a backward for the loss function\n",
    "    \n",
    "    # step 5 (final): compute train epoch loss and metrics\n",
    "\n",
    "    ################\n",
    "    print(f'Train loss: {rolling_loss} | Train metric: {rolling_metric}')\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model:nn.Module,\n",
    "    val_dataloader:torch.utils.data.DataLoader, \n",
    "    loss_fn:torch.nn.modules.loss, \n",
    "    metrics, # define you type here  \n",
    "    device:torch.device):\n",
    "    ''' method to perform evaluation step '''\n",
    "\n",
    "    rolling_loss = 0.\n",
    "    rolling_metric = 0.\n",
    "    for input, target in val_dataloader: \n",
    "        input, target = input.to(device), target.to(device)\n",
    "        ################\n",
    "\n",
    "        # YOUR CODE HERE \n",
    "\n",
    "        # step 1: pass input tensor through the model\n",
    "\n",
    "        # step 2: compute loss and metrics \n",
    "    \n",
    "    ################\n",
    "    print(f'Validation loss: {rolling_loss} | Validation metric: {rolling_metric}')\n",
    "    \n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, loss_fn, metrics, optimiser, device, epochs):\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i+1}\")\n",
    "        train_single_epoch(model, train_dataloader, loss_fn, optimiser, device)\n",
    "        evaluate(model, val_dataloader, loss_fn, metrics, device)\n",
    "    print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Define your loss function, optimization algorithm, and a metric function(s)\n",
    "loss_fn = # TODO\n",
    "optimiser = # TODO\n",
    "metrics = # TODO\n",
    "\n",
    "train_dataloader = create_data_loader(train_dataset, BATCH_SIZE)\n",
    "val_dataloader = create_data_loader(val_dataset, BATCH_SIZE)\n",
    "test_dataloader = create_data_loader(test_dataset, BATCH_SIZE)\n",
    "\n",
    "\n",
    "# train model\n",
    "train(cnn, train_dataloader, val_dataloader, loss_fn, metrics, optimiser, device, EPOCHS)\n",
    "\n",
    "# save model\n",
    "torch.save(cnn.state_dict(), \"feedforwardnet.pth\")\n",
    "print(\"Trained feed forward net saved at feedforwardnet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# evaluate your model on test split\n",
    "\n",
    "################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "1) Write a litter summary of the audio transformation methods (1-2 sentences), and explore them on your own\n",
    "2) Create a table with the model performance comparison. The table should include short model summary (ex. 4xConvBlocks - Flatten - Linear - Softmax), comments about hyperparameters (ex. ConvBlocks: [64, 64, 64, 64] - Dropout: 0.2, etc.) and metrics values for validation and test datasets.\n",
    "3) Train baseline model with 4x ConvBlocks - Flatten - Linear - Softmax\n",
    "4) Train baseline model with BatchNorm\n",
    "5) Train baseline model with BatchNorm and Dropout\n",
    "6) Train 5 model models with different hyperparameters\n",
    "7) Add all models to a table \n",
    "8) Write an explanation to the table and indicate the best model parameters and architecture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
